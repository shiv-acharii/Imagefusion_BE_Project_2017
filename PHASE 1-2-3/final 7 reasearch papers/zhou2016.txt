
Abstract:

In order to achieve perceptually better fusion of infrared (IR) and visible images than conventional pixel-level fusion algorithms based on multi-scale decomposition (MSD), we present a novel multi-scale fusion method based on a hybrid multi-scale decomposition (hybrid-MSD). The proposed hybrid-MSD transform decomposes the source images into multi-scale texture details and edge features by jointly using multi-scale Gaussian and bilateral ?lters. This transform enables to better capture important multi-scale IR spectral features and separate ?ne-scale texture details from large-scale edge features. As a result, we can use it to achieve better fusion result for human visual perception than those obtained from conventional multi-scale fusion methods, by injecting the multi-scale IR spectral features into the visible image, while preserving (or properly enhancing) important perceptual cues of the background scenery and details from the visible image. In the decomposed information fusion process, three different combination algorithms are adaptively used in accordance to different scale levels (i.e., the small-scale levels, the large-scale levels and the base level). A regularization parameter is introduced to control the relative amount of IR spectral information injected into the visible image in a soft manner, which can be adjusted further depending on user preferences. Moreover, by testing different settings of the parameter, we demonstrate that injecting a moderate amount of IR spectral information with this parameter can actually make the fused images visually better for some infrared and visible source images. Experimental results of both objective assessment and subjective evaluation by human observers also prove the superiority of the proposed method compared with conventional MSD-based fusion methods.


1. Introduction

Infrared (IR) and visible image fusion is an important technique in multi-sensor information fusion applications. Since IR sensors are able to capture thermal information in a scene that is not directly seen by human eyes, they can more clearly detect some objects in low-light, occlusion and adverse weather conditions. Visible imagery normally provides more details of the scene in the visible spectrum, and also presents more natural intensities and contrasts that are con-sistent with human visual perception. Integrating IR and visible infor-mation into a fused image allows us to construct a more complete and accurate description of the scene. However, due to heat emissions and differing spectral sensitivities, the relative luminance response in the IR spectrum is quite inconsistent with that in the visible spectrum, which makes the IR imagery hard to be interpreted. As a result, the fu-sion result of IR and visible imageries may also be visually unpleasing
for human observers. Therefore, besides determining the best way to take full advantage of all information of the two source images in the fusion process, a more significant task should be to make the fused image easy to be interpreted, and thus can lead to better situation awareness.

A more common approach for pixel-level image fusion is con-ducted in grayscale space based on various multi-scale decompo-sition (MSD) transforms, e.g., Laplacian pyramid (LAP) [4], gradient pyramid (GP) [5], wavelet transform [6] and support vector trans-form [7]. For these MSD-based fusion methods, MSD transforms are performed first on each source image to obtain different subbands containing the decomposed information of different frequencies or orientations. The corresponding subbands of all source images are then combined together based on certain fusion rules. Finally, the fused images can be produced by inverse MSD transforms. To merge thermal and visual images, Toet et al. [8] propose one of the earliest MSD transform named contrast pyramid or ratio of low pass pyramid (ROLP). The ROLP is formed as a ratio of each level of Gaussian pyra-mid to its next coarser-level one. Fusion with ROLP enables to identify and preserve important high-contrast details that are more relevant to visual perception.

Other more complex MSD transforms including dual-tree complex wavelet (DT-CWT), curvelet and contourlet have also been success-fully applied in image fusion [9–13]. These MSD transforms achieve better properties of shift invariance and directional selectivity, which are essential for multispectral image fusion since they enable to correctly transfer more directional details into the fused image. However, since the IR imagery usually presents large differences in relative luminance response (and thus may generate coarser-scale in-formation, rather inconsistent contrasts or different edges and con-tours, etc.) against that of the corresponding visible imagery, directly combining the two image salient features based on these conven-tional MSDs may clutter the image content in the fusion result and make it visually unpleasing (e.g., blurring of details, introducing un-natural contrasts or visual artifacts).

In this paper, we present a novel multi-scale fusion method based on a hybrid MSD transform (hybrid-MSD) to achieve better fusion re-sults for human visual perception. Unlike the previous MSD trans-forms that attempt to capture more directional information with comparatively more complex filters, the hybrid-MSD decomposes the source image into texture details and edge features at multiple scales by jointly using multi-scale Gaussian and bilateral filters. In a perceptual evaluation of different image fusion schemes, Toet et al. [14] indicate that the IR imagery serves best for target detection and recognition, whereas the visible imagery contributes most to global scene awareness. Our method manages to employ the hybrid-MSD as well as a novel asymmetrical multi-scale fusion scheme to in-ject the important IR spectral features into the visible image, while preserving (or properly enhancing) important perceptual cues of the background scenery and details captured from the visible spectrum. Thus, it would lead to perceptually better fusion results for human interpretation.


